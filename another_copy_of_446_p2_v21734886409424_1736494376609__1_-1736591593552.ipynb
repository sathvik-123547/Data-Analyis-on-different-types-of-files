{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHj92VDfTEPO"
      },
      "source": [
        "Your name: //IGNORE\n",
        "\n",
        "Your student ID number: //IGNORE\n",
        "\n",
        "Shared link to this notebook: //IGNORE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoFerkqpTEPQ"
      },
      "source": [
        "# Programming Assignment 2 (P2) for COMPSCI 446, Search Engines\n",
        "\n",
        "_This is an updated version of the notebook for P2. Please watch Piazza for announcements of new versions and/or check back periodically to see if it has been updated._\n",
        "\n",
        "_As of this version of the notebook, the autograder is set up in Gradescope._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVSv_Zs7TEPR"
      },
      "source": [
        "The purpose of this project is to explore and implement some of the effectiveness measures discussed in class and in the text (Section 8.4). You will implement a function called eval that will be invoked as follows:\n",
        "```python\n",
        "eval(trecrunFile, qrelsFile, outputFile)\n",
        "```\n",
        "\n",
        "where `trecrunFile` and `qrelsFile` are files that we will provide to you and `outputFile` is where you will print the evaluation results of the trecrun file.\n",
        "\n",
        "A trecrun file contains the actual system runs that you are going to evaluate. It is a text file with six space-separated columns on every line (note that for obvious reasons, no column can contain spaces):\n",
        "- The first column is the query name (aka query id)\n",
        "- The second column is unused and should always contain \"skip\" (for historical reasons)\n",
        "- The third column is a document identifier (“docid”)\n",
        "- The fourth column is the rank of that document for that query in this run\n",
        "- The fifth column is the score from the retrieval model.\n",
        "- The sixth column is some text to describe the run itself, normally the same for every line in the file.\n",
        "\n",
        "The qrels file contains judgment information: given a query and a document, is the document relevant to the query? It is another space-separated text file:\n",
        "- The first column is the query name/id (corresponding to the query name/id in the trecrun files)\n",
        "- The second column is unused (it is present for historical reasons; you won't need to do anything with it except be sure you read it to get to the remaining columns)\n",
        "- The third column is a document identifier (“docid”)\n",
        "- The fourth column is a number representing the relevance of the document, either 0 for non-relevant, or positive for relevant.\n",
        "\n",
        "We will provide a common qrels file as well as several trecrun files and sample output files. We will also provide copies of the queries in case you find it useful, though they aren't needed.\n",
        "\n",
        "Note that for many of the query-docid pairs in the trecrun file, you will find a corresponding pair in the qrels file, helping you to evaluate how well the retrieval model did for that query. However, as discussed in class, **large numbers of query-docid pairs will be unjudged, so will not appear in the qrels file. When you encounter that, you should assume that the query-docid pair is non-relevant** (i.e., has a relevance score of zero).\n",
        "\n",
        "Your code will read in the provided trecrun and qrels files. It will then generate evaluation scores and print them in a particular format (see below) to the indicated output filename.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup"
      ],
      "metadata": {
        "id": "t4Xvai7VPAdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "version = 2 # DO NOT MODIFY. If notebook does not match with autograder version, many tests are likely to fail!!"
      ],
      "metadata": {
        "id": "hWAYQYm7AiFQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We first execute the following to connect to Google Drive (you will be prompted repeatedly for access to your Google Drive; please give it permission) and download copies of the sample files listed above. You should not need to make any modifications to the code, though if you want to use a slightly different path in Google Drive, you can modify the appropriate data_path value. (The autograder will not use your Google Drive.)"
      ],
      "metadata": {
        "id": "VaALU6IfSdcP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZjCDAZdyTEPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b8c3e7-3e8b-49fd-b318-012a62480ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import string\n",
        "import gzip\n",
        "import re\n",
        "\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    in_colab = True\n",
        "except ImportError:\n",
        "    in_colab = False\n",
        "\n",
        "\n",
        "# You are more than welcome to code some helper functions.\n",
        "# But do note that we are only grading functions that are coded in the template files.\n",
        "\n",
        "\n",
        "# Connect to Google Drive and download copies of the sample files listed above.\n",
        "# Please allow the access to your Google Drive or the following dataset loader will fail.\n",
        "# (The autograder will not use your Google Drive.)\n",
        "if in_colab:\n",
        "  drive.mount(\"/content/drive/\") ## DO NOT MODIFY THIS LINE\n",
        "  data_path = \"/content/drive/MyDrive/COMPSCI446\" ## CREATE THIS FOLDER/DIRECTORY IN YOUR GOOGLE DRIVE\n",
        "else:\n",
        "  data_path = \"./data/\"  ## DO NOT MODIFY THIS LINE. CHANGING THIS LINE WOULD RESULT IN FAIL OF AUTOGRADER TESTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYjbvdIPt2pO",
        "outputId": "b893d9c6-c331-49ae-889b-861c97063968"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now download the files needed for this assignment to your Google Drive in the path declared in <em>data_path</em> so that you can work with them in the rest of the notebook. We will not bother loading the files if they have already been loaded to your Google Drive, so this should be a one-time effort.\n",
        "\n",
        "**NOTE**: This code will not create a new folder for you. It assumes that you have a folder called COMPSCI446 at the top level of your Google Drive. You can create that folder and everything will work fine. If you want to create something different (e.g., COMPSCI446-P2 or COMPSCI446/P2) then create that folder and edit the data_path line in the cell above."
      ],
      "metadata": {
        "id": "hoA5bSTsGl2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "def download_file(file_path: str, zip: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Download the file to proper location in mounted Google Drive.\n",
        "\n",
        "    Args:\n",
        "        file_path: the location of file we want to download\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    webloc = \"https://cs.umass.edu/~allan/cs446/\"\n",
        "\n",
        "\n",
        "    data_info = Path(data_path)\n",
        "    if not data_info.exists() or not data_info.is_dir():\n",
        "      print(f\"Google folder \\\"{data_path}\\\" is not present or not a folder. Can't download file {file_path}.\")\n",
        "      return\n",
        "\n",
        "    local_google_drive_path = os.path.join(data_path,file_path)\n",
        "    local_file = Path(local_google_drive_path)\n",
        "    if local_file.is_file():\n",
        "        print(f\"File \\\"{file_path}\\\" already exists, not downloading.\")\n",
        "    else:\n",
        "        print(f\"Cannot find \\\"{file_path}\\\" so downloading it.\")\n",
        "        urllib.request.urlretrieve(webloc + file_path, local_google_drive_path)\n",
        "        print(\"Done\")\n",
        "\n",
        "    if zip:\n",
        "      with zipfile.ZipFile(local_google_drive_path, 'r') as zip_ref:\n",
        "        print(f\"Unzipping \\\"{file_path}\\\"\")\n",
        "        zip_ref.extractall(data_path)\n",
        "\n",
        "\n",
        "\n",
        "P2_train_path = \"P2train.zip\"\n",
        "P2_output = \"P2train-output.zip\"\n",
        "\n",
        "download_file(P2_train_path)\n",
        "download_file(P2_output)"
      ],
      "metadata": {
        "id": "phggEbZAH6uH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca556bb-a621-4256-f0be-5262ca211592"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot find \"P2train.zip\" so downloading it.\n",
            "Done\n",
            "Unzipping \"P2train.zip\"\n",
            "Cannot find \"P2train-output.zip\" so downloading it.\n",
            "Done\n",
            "Unzipping \"P2train-output.zip\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we're testing only the end result of your program -- i.e., the output file -- after it runs, parses, and evaluates the `trecrun` and `qrels` file correctly. Therefore, the only function that you **MUST** have is the eval function:\n",
        "```python\n",
        "def eval(trecrunFile, qrelsFile, outputFile):\n",
        "  pass\n",
        "```\n",
        "Other than providing a functional eval() function, **you are free to define supporting functions, classes, and use standard python libraries to implement the `eval` function in any way that you like**.\n",
        "\n",
        "**However**, we know that different people have different code structures in mind, but for ease of debugging and readability purposes, we suggest you follow the provided general outline below for your program. If you prefer another way to structure the program, you can go ahead and replace the outline with yours. As mentioned, the only exception is the `eval` function where you have to make sure that you don't modify the definition at the bottom of this notebook or the expected format of the inputs/outputs (or else the autograder will fail and your grade will suffer)."
      ],
      "metadata": {
        "id": "f5zVWqTc_nqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Loading the data"
      ],
      "metadata": {
        "id": "M78UAoC6IBXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Creating a data structure\n",
        "\n",
        "The qrels and trecrun files can get very large. Iterating through the entire file for each run will take minutes, if not longer, and therefore is impractical. In practice, it is more efficient to implement a data structure to hold `qrels` and  `trecrun` information.\n",
        "\n",
        "We'll highlight once more that this is a suggested way to handle the qrels and trecrun files. You _unquestionably_ want to store them in some data struture, but what you call it and how you use it is entirely up to you. We're providing a suggestion that may help you."
      ],
      "metadata": {
        "id": "hr5duhuoLBzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QueryInfo:\n",
        "  \"\"\"\n",
        "  QueryInfo class: store the trecrun and qrels data of each query\n",
        "  \"\"\"\n",
        "\n",
        "  #########\n",
        "  ##\n",
        "  ## Add any attributes and methods necessary\n",
        "  ##\n",
        "  #########\n",
        "\n",
        "\n",
        "  def __init__(self):\n",
        "    self.trecrun = []  # List to store trecrun data (rankings for each document)\n",
        "    self.qrels = {}\n",
        "    self.ranked_list = []   # Dictionary to store qrels data (docid: relevance score)"
      ],
      "metadata": {
        "id": "n0RZSkVqVaA6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For easier access to qrel and trecrun information during evaluation measure calculations, we suggest you store all query information in a dictonary mapping query ids to the corresponding query object. Note that this example assumes that query ids are strings (rather than integers) which is a good assumption."
      ],
      "metadata": {
        "id": "UgQzke3mRlIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = dict[str, QueryInfo]() # dictionary to store query_id/QueryInfo mappings"
      ],
      "metadata": {
        "id": "4M93-nPXR9wJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Parsing trecruns\n",
        "Now we can use the data structure above to store the trecrun rankings:"
      ],
      "metadata": {
        "id": "x34MEQrcINNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_trecrun(trecRunFile: str, queriesDict: dict[str, QueryInfo]) -> None:\n",
        "  \"\"\"\n",
        "  Read the trecrun file data and store the ranking lists for each query in a corresponding QueryInfo object.\n",
        "\n",
        "  Args:\n",
        "        trecRunFile: path to where the trecrun file is stored\n",
        "        queriesDict: dictionary to store QueryInfos, mapping each query's id to its corresponding QueryInfo object\n",
        "  \"\"\"\n",
        "\n",
        "  #########\n",
        "  ##\n",
        "  ## Implement the function here\n",
        "  ##\n",
        "  #########\n",
        "\n",
        "  with open(trecRunFile, 'r') as file:\n",
        "    for line in file:\n",
        "        query_id, _, docid, rank, score, run_name = line.strip().split()\n",
        "        if query_id not in queriesDict:\n",
        "            queriesDict[query_id] = QueryInfo()\n",
        "        queriesDict[query_id].trecrun.append({\n",
        "            'docid': docid,\n",
        "            'rank': int(rank),\n",
        "            'score': float(score),\n",
        "            'run_name': run_name\n",
        "        })\n",
        "        queriesDict[query_id].ranked_list.append(docid)  # Populate ranked_list\n"
      ],
      "metadata": {
        "id": "8vOQ1zL8IZFf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Parsing qrels\n",
        "Add the qrels data to the data structure as well:"
      ],
      "metadata": {
        "id": "zfn5tXJUPedJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_qrels(qrelsFile: str, queriesDict: dict[str, QueryInfo]) -> None:\n",
        "  \"\"\"\n",
        "  Read the qrels file data and store the relevance judgements for each query in a corresponding QueryInfo object.\n",
        "\n",
        "  Args:\n",
        "        qrelsFile: path to where the qrels file is stored\n",
        "        queriesDict: dictionary to store QueryInfos, mapping each query's id to its corresponding QueryInfo object\n",
        "  \"\"\"\n",
        "\n",
        "  #########\n",
        "  ##\n",
        "  ## Implement the function here\n",
        "  ##\n",
        "  #########\n",
        "\n",
        "  with open(qrelsFile, 'r') as file:\n",
        "    for line in file:\n",
        "        query_id, _, docid, relevance = line.strip().split()\n",
        "        if query_id not in queriesDict:\n",
        "            queriesDict[query_id] = QueryInfo()\n",
        "        queriesDict[query_id].qrels[docid] = int(relevance)"
      ],
      "metadata": {
        "id": "eV9MOTaKPgqd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Effectiveness measures"
      ],
      "metadata": {
        "id": "q5x8oiqtLenP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We expect you to implement the following effectiveness measures and later (in the ```eval``` function) report values on them for any trecrun file that is provided. **If the file contains multiple queries (which it usually will), you will need to calculate measures for each query and then also produce the arithmetic mean of each evaluation measure across all queries in the trecrun file.**\n",
        "\n",
        "Note that you are producing these numbers for every query that occurs in the trecrun file. There will probably be queries listed in the qrels file that are not in the trecrun file; ignore them.\n",
        "\n",
        "The measures you will need to implement for each query are (please use the formulations described in the textbook, and macro-averaging where given the choice):\n",
        "\n",
        "- `numRel` is the total number of relevant documents (relevance score above zero) that are listed in the qrels file for this query.\n",
        "- `relFound` is the count of those relevant documents that also appear in the ranked list of documents.\n",
        "- `RR` (Reciprocal Rank); if relFound is zero, then report “0” for this measure.\n",
        "- `P@13` (Precision @ 13)\n",
        "- `R@13` (Recall @ 13); if numRel is zero, then report \"0\" for this measure.\n",
        "- `F1@13`; if either precision or recall is zero, then report \"0\" for this measure.\n",
        "- `AP` (Average Precision); if numRel is zero, report \"0\".\n",
        "- `nDCG@23` (there are multi-value relevance judgments in the data – 0,1,2 – though some queries only have 0/1 judgments). If numRel is zero, then report 0 for nDCG. Hint: A key thing to note in calculating nDCG is that the \"ideal\" DCG cannot be calculated using just the retrieved documents for a given query. Think about why so you avoid that mistake (it was a midterm question, too). Note: use the DCG equation on page 319 of the textbook that just uses $rel_i$, not the equation that is in footnote 10 of page 320 that uses $2^{rel_i}-1$.\n",
        "- `BPREF`. If numRel is zero, report \"0\". Use the formula $\\frac{1}{R}\\Sigma_{d_r}(1-\\frac{N_{d_r}}{R})$.\n",
        "\n",
        "\n",
        "*Extra Credit:*\n",
        "\n",
        "- `P@29R` is precision when recall is 29 percent (note that this is percent recall and not rank 29); if numRel is zero, report \"0\". Remember that this requires that you interpolate to figure out the precision -- it is the maximum precision at any recall value $R'$ where $R' \\geq R$. See section 8.4.2 in the textbook or the content of the evaluation lecture.\n",
        "- `P@R` where $R$ is the number of relevant documents for this query; if numRel is zero, report \"0\".\n",
        "\n",
        "For the extra credit items, we are not providing sample output and when you submit your assignment, you will only be told that the autograder _found_ your extra credit (or not). As a hint, though, in the BM25 run of msmarcosmall, for query 390360 you should find that P@29R is 0.4400 and that P@R is 0.3429. And for query 118440, you should get 0.0000 and 0.0461, respectively."
      ],
      "metadata": {
        "id": "2HUL96L5L4PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are possible stubs for the evaluation functions. One last time, note that you are not required to implement them this way, but you may find this way of thinking about it helpful.\n",
        "\n",
        "These stubs assume that you have identified the \"query info\" for the query you're considering and that you pass that in. The suggested variable ```queries``` was defined earlier as a dictionary that mapped query ids to this query information."
      ],
      "metadata": {
        "id": "LfsI5msi9cMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numRel(query: QueryInfo) -> int:\n",
        "    \"\"\"\n",
        "    Calculate total number of relevant documents that are listed in the qrels file for the given query.\n",
        "\n",
        "    Args:\n",
        "        query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: number of relevant documents in the corpus for this query\n",
        "    \"\"\"\n",
        "    # Count documents in qrels with a relevance score greater than 0\n",
        "    return sum(1 for relevance in query.qrels.values() if relevance > 0)\n"
      ],
      "metadata": {
        "id": "C2pofBWoNImA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relFound(query: QueryInfo) -> int:\n",
        "    \"\"\"\n",
        "    Calculate the number of relevant documents retrieved in the ranked list for the given query.\n",
        "\n",
        "    Args:\n",
        "        query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: number of relevant documents retrieved in the ranked list for this query\n",
        "    \"\"\"\n",
        "    # Count documents in the ranked list that are relevant (relevance score > 0 in qrels)\n",
        "    return sum(1 for docID in query.ranked_list if query.qrels.get(docID, 0) > 0)\n"
      ],
      "metadata": {
        "id": "w4NASCYZNOqD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RR(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the reciprocal rank for the given query.\n",
        "\n",
        "    Args:\n",
        "        query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the reciprocal rank score of this query\n",
        "    \"\"\"\n",
        "    # Iterate through the ranked list and find the first relevant document\n",
        "    for rank, docID in enumerate(query.ranked_list, start=1):\n",
        "        if query.qrels.get(docID, 0) > 0:  # Relevance > 0 means the document is relevant\n",
        "            return 1.0 / rank\n",
        "    return 0.0  # If no relevant document is found, return 0\n"
      ],
      "metadata": {
        "id": "rjaxZGv7NZKW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def P_13(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Precision at 13 for the given query.\n",
        "\n",
        "    Args:\n",
        "        query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the P@13 score of this query\n",
        "    \"\"\"\n",
        "    # Take the top 13 documents from the ranked list\n",
        "    top_13 = query.ranked_list[:13]\n",
        "    # Count the relevant documents within the top 13\n",
        "    relevant_count = sum(1 for docID in top_13 if query.qrels.get(docID, 0) > 0)\n",
        "    # Precision is the fraction of relevant documents out of the total 13 (or fewer if fewer documents are present)\n",
        "    return relevant_count / len(top_13) if top_13 else 0.0\n"
      ],
      "metadata": {
        "id": "T_fB0DDnNfgT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def R_13(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Recall at 13 for the given query.\n",
        "\n",
        "    Args:\n",
        "        query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the R@13 score of this query\n",
        "    \"\"\"\n",
        "    # Total number of relevant documents for the query\n",
        "    total_relevant = numRel(query)\n",
        "    if total_relevant == 0:\n",
        "        return 0.0  # Avoid division by zero, recall is 0 if no relevant documents exist\n",
        "\n",
        "    # Take the top 13 documents from the ranked list\n",
        "    top_13 = query.ranked_list[:13]\n",
        "    # Count the relevant documents within the top 13\n",
        "    relevant_in_top_13 = sum(1 for docID in top_13 if query.qrels.get(docID, 0) > 0)\n",
        "\n",
        "    # Recall is the fraction of relevant documents in the top 13 out of all relevant documents\n",
        "    return relevant_in_top_13 / total_relevant\n"
      ],
      "metadata": {
        "id": "rp41Qq_lNgD1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def F1_13(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    Calculate F1 score at 13 for the given query.\n",
        "\n",
        "    Args:\n",
        "        query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the F1@13 score of this query\n",
        "    \"\"\"\n",
        "    # Calculate Precision at 13\n",
        "    precision = P_13(query)\n",
        "    # Calculate Recall at 13\n",
        "    recall = R_13(query)\n",
        "\n",
        "    # Avoid division by zero: F1 is 0 if either precision or recall is 0\n",
        "    if precision == 0 or recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # F1 Score formula: 2 * (precision * recall) / (precision + recall)\n",
        "    return 2 * (precision * recall) / (precision + recall)\n"
      ],
      "metadata": {
        "id": "rgQTOa9hNgjZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AP(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Average Precision for the given query.\n",
        "\n",
        "    Args:\n",
        "        query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the average precision score of this query\n",
        "    \"\"\"\n",
        "    # Extract relevant data from the query object\n",
        "    ranked_list = query.ranked_list  # Assuming this holds the ranked list of document IDs\n",
        "    qrels = query.qrels  # Assuming this holds the relevance judgments for the query\n",
        "\n",
        "    num_rel = sum(1 for doc in qrels if qrels[doc] > 0)  # Total number of relevant documents\n",
        "    if num_rel == 0:  # No relevant documents, AP is 0\n",
        "        return 0.0\n",
        "\n",
        "    sum_precision = 0.0  # Sum of precisions at ranks with relevant documents\n",
        "    num_relevant_found = 0  # Counter for relevant documents found in the ranked list\n",
        "\n",
        "    for rank, doc_id in enumerate(ranked_list, start=1):\n",
        "        if doc_id in qrels and qrels[doc_id] > 0:  # Check if the document is relevant\n",
        "            num_relevant_found += 1\n",
        "            precision_at_k = num_relevant_found / rank\n",
        "            sum_precision += precision_at_k\n",
        "\n",
        "    return sum_precision / num_rel  # Average precision\n"
      ],
      "metadata": {
        "id": "XbIlYp8FNhgt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NDCG_23(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    Calculate nDCG at 23 for the given query.\n",
        "\n",
        "    Args:\n",
        "          query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the nDCG at 23 score of this query\n",
        "    \"\"\"\n",
        "    ranked_list = query.ranked_list[:23]  # Limit to top 23 documents\n",
        "    qrels = query.qrels  # Relevance judgments\n",
        "\n",
        "    def dcg(relevance_scores):\n",
        "        return sum(\n",
        "            (2**rel - 1) / math.log2(idx + 2) for idx, rel in enumerate(relevance_scores)\n",
        "        )\n",
        "\n",
        "    relevance_scores = [qrels.get(doc_id, 0) for doc_id in ranked_list]\n",
        "    ideal_relevance_scores = sorted(\n",
        "        (score for score in qrels.values() if score > 0), reverse=True\n",
        "    )[:23]\n",
        "\n",
        "    dcg_val = dcg(relevance_scores)\n",
        "    idcg_val = dcg(ideal_relevance_scores)\n",
        "\n",
        "    return dcg_val / idcg_val if idcg_val > 0 else 0.0\n"
      ],
      "metadata": {
        "id": "44Z9r144fwNB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BPREF(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    Calculate BPREF for the given query.\n",
        "\n",
        "    Args:\n",
        "          query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the BPREF score of this query\n",
        "    \"\"\"\n",
        "    ranked_list = query.ranked_list  # Ranked list of document IDs\n",
        "    qrels = query.qrels  # Relevance judgments\n",
        "\n",
        "    relevant_docs = [doc for doc, rel in qrels.items() if rel > 0]\n",
        "    non_relevant_docs = [doc for doc, rel in qrels.items() if rel == 0]\n",
        "\n",
        "    num_rel = len(relevant_docs)\n",
        "    if num_rel == 0:\n",
        "        return 0.0\n",
        "\n",
        "    num_non_rel = len(non_relevant_docs)\n",
        "    sum_score = 0.0\n",
        "\n",
        "    for idx, doc_id in enumerate(ranked_list):\n",
        "        if doc_id in relevant_docs:\n",
        "            non_rel_seen = sum(\n",
        "                1 for prev_doc in ranked_list[:idx] if prev_doc in non_relevant_docs\n",
        "            )\n",
        "            sum_score += 1 - (non_rel_seen / min(num_rel, num_non_rel))\n",
        "\n",
        "    return sum_score / num_rel\n"
      ],
      "metadata": {
        "id": "7AGJBvf43aB7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These next two stubs are for the optional extra credit."
      ],
      "metadata": {
        "id": "6tev-pjuNFDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def P_29R(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    (Extra Credit) Calculate Precision at 29% recall for the given query.\n",
        "\n",
        "    Args:\n",
        "          query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the Precision at 29% recall score of this query\n",
        "    \"\"\"\n",
        "    ranked_list = query.ranked_list  # Ranked list of document IDs\n",
        "    qrels = query.qrels  # Relevance judgments\n",
        "\n",
        "    relevant_docs = [doc for doc, rel in qrels.items() if rel > 0]\n",
        "    num_rel = len(relevant_docs)\n",
        "\n",
        "    if num_rel == 0:\n",
        "        return 0.0\n",
        "\n",
        "    target_recall_count = math.ceil(0.29 * num_rel)\n",
        "    relevant_found = 0\n",
        "    precision_at_29r = 0.0\n",
        "\n",
        "    for rank, doc_id in enumerate(ranked_list, start=1):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_found += 1\n",
        "            if relevant_found == target_recall_count:\n",
        "                precision_at_29r = relevant_found / rank\n",
        "                break\n",
        "\n",
        "    return precision_at_29r\n"
      ],
      "metadata": {
        "id": "slEgWM4a0xzG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PatR(query: QueryInfo) -> float:\n",
        "    \"\"\"\n",
        "    (Extra Credit) Calculate Precision at the number of relevant documents for the given query.\n",
        "\n",
        "    Args:\n",
        "          query: input query holding the qrels and ranked list information\n",
        "\n",
        "    Returns: single number representing the Precision at R score of this query\n",
        "    \"\"\"\n",
        "    ranked_list = query.ranked_list  # Ranked list of document IDs\n",
        "    qrels = query.qrels  # Relevance judgments\n",
        "\n",
        "    relevant_docs = [doc for doc, rel in qrels.items() if rel > 0]\n",
        "    num_rel = len(relevant_docs)\n",
        "\n",
        "    if num_rel == 0:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_found = 0\n",
        "    for rank, doc_id in enumerate(ranked_list, start=1):\n",
        "        if doc_id in relevant_docs:\n",
        "            relevant_found += 1\n",
        "        if relevant_found == num_rel:\n",
        "            return relevant_found / rank\n",
        "\n",
        "    return 0.0\n"
      ],
      "metadata": {
        "id": "b3ALpuqlNKkj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Evaluation and Output"
      ],
      "metadata": {
        "id": "6La4ZOuMO1BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each input trecrun file, we want you to report the calculated measures in the output file following the format given below. If the trecrun file contains multiple queries, you will need to calculate and report measures for each query. At the end of the evaluation output file, you should also include the _sum_ of the values for numRel and relFound, but the (arithmetic) _mean_ value of each of the other measures, averaged (or summed) across all queries in the trecrun file. In each case you will calculate scores per query and the mean or summed scores (so, for example, you will calculate AP for each query and the mean AP -- i.e., MAP -- for each TREC run overall). Your output format for each query should be exactly as follows, except that the amount of whitespace you use to separate fields is your choice. Each line will look like:\n",
        "```\n",
        "measure    queryid    score\n",
        "```\n",
        "Where measure is one of those given above. For most measures, format the scores to include exactly four digits after the decimal place. For counts, just report the integer number. For example, for query 302 you might get (these numbers are random, so not meaningful; plus the last two are extra credit, so you might not have them):\n",
        "```\n",
        "\n",
        "numRel        302    23\n",
        "relFound      302    20\n",
        "RR            302    1.0000\n",
        "P@13          302    0.6667\n",
        "R@13          302    0.1846\n",
        "F1@13         302    0.2667\n",
        "NDCG@23       302    0.3118\n",
        "AP            302    0.1531\n",
        "BPREF         302    0.3333\n",
        "P@29R         302    0.5000\n",
        "P@R           302    0.1111\n",
        "```\n",
        "For the final average across queries, replace RR with MRR and AP with MAP since those are the normal names for the means. Also, for the final averages, use \"all\" in place of the queryname. For example,\n",
        "```\n",
        "MRR           all   0.9438\n",
        "MAP           all   0.0581\n",
        "```"
      ],
      "metadata": {
        "id": "DtyopPnrSaJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_measures(outputFile: str, results) -> None:\n",
        "    \"\"\"\n",
        "    Write all calculated measures into the output file.\n",
        "\n",
        "    Args:\n",
        "          outputFile: path in which the output file should be stored\n",
        "          results: contains all calculated measures for all queries in the trecrun file\n",
        "    \"\"\"\n",
        "    with open(outputFile, \"w\") as f:\n",
        "        for query_id, measures in results.items():\n",
        "            for measure_name, score in measures.items():\n",
        "                # Write each measure in the required format\n",
        "                if isinstance(score, float):\n",
        "                    f.write(f\"{measure_name}\\t{query_id}\\t{score:.4f}\\n\")\n",
        "                else:  # For integer measures like numRel, relFound\n",
        "                    f.write(f\"{measure_name}\\t{query_id}\\t{score}\\n\")\n",
        "\n",
        "        # Write overall averages or totals using \"all\" as the query_id\n",
        "        if \"all\" in results:\n",
        "            for measure_name, score in results[\"all\"].items():\n",
        "                if isinstance(score, float):\n",
        "                    f.write(f\"{measure_name}\\tall\\t{score:.4f}\\n\")\n",
        "                else:\n",
        "                    f.write(f\"{measure_name}\\tall\\t{score}\\n\")\n"
      ],
      "metadata": {
        "id": "z_F6VyrgDKWg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together\n",
        "Use all the definitions and functions you implemented above to complete the definition of the  `eval` function, regardless of whether you used our suggestions or structured things in your own way.\n",
        "\n",
        "Remember that the expected behavior of the `eval` function is to take as input paths to trecrun and qrel files, calculate all the measures for each query in the trecrun file, and report the calculated measures and their averages in an output file in the path included in the input to `eval`.\n",
        "\n",
        "Make sure not to modify the definition of the `eval` function or the expected format of the inputs/outputs (otherwise the autograder will fail). The body of the ``eval`` function will depend on how you implemented the details above."
      ],
      "metadata": {
        "id": "R48sIqT3O7zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(trecrunFile: str, qrelsFile: str, outputFile: str) -> None:\n",
        "    \"\"\"\n",
        "    Calculate and report all the measures for each query in the trecrun file.\n",
        "\n",
        "    Args:\n",
        "          trecRunFile: path to where the trecrun file is stored\n",
        "          qrelsFile: path to where the qrels file is stored\n",
        "          outputFile: path in which the output file should be stored\n",
        "    \"\"\"\n",
        "    queries = {}  # Dictionary to store query_id -> QueryInfo mappings\n",
        "\n",
        "    # Parse the trecrun and qrels files\n",
        "    read_trecrun(trecrunFile, queries)\n",
        "    read_qrels(qrelsFile, queries)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Calculate measures for each query\n",
        "    for query_id, query_info in queries.items():\n",
        "        measures = {}\n",
        "        measures[\"numRel\"] = numRel(query_info)\n",
        "        measures[\"relFound\"] = relFound(query_info)\n",
        "        measures[\"RR\"] = RR(query_info)\n",
        "        measures[\"P@13\"] = P_13(query_info)\n",
        "        measures[\"R@13\"] = R_13(query_info)\n",
        "        measures[\"F1@13\"] = F1_13(query_info)\n",
        "        measures[\"AP\"] = AP(query_info)\n",
        "        measures[\"nDCG@23\"] = NDCG_23(query_info)\n",
        "        measures[\"BPREF\"] = BPREF(query_info)\n",
        "        # Optional extra credit measures\n",
        "        measures[\"P@29R\"] = P_29R(query_info)\n",
        "        measures[\"P@R\"] = PatR(query_info)\n",
        "        results[query_id] = measures\n",
        "\n",
        "    # Calculate averages and totals across all queries\n",
        "    all_measures = {}\n",
        "    total_queries = len(queries)\n",
        "    all_measures[\"numRel\"] = sum(results[q][\"numRel\"] for q in results)\n",
        "    all_measures[\"relFound\"] = sum(results[q][\"relFound\"] for q in results)\n",
        "    all_measures[\"MRR\"] = sum(results[q][\"RR\"] for q in results) / total_queries\n",
        "    all_measures[\"MAP\"] = sum(results[q][\"AP\"] for q in results) / total_queries\n",
        "    all_measures[\"P@13\"] = sum(results[q][\"P@13\"] for q in results) / total_queries\n",
        "    all_measures[\"R@13\"] = sum(results[q][\"R@13\"] for q in results) / total_queries\n",
        "    all_measures[\"F1@13\"] = sum(results[q][\"F1@13\"] for q in results) / total_queries\n",
        "    all_measures[\"nDCG@23\"] = sum(results[q][\"nDCG@23\"] for q in results) / total_queries\n",
        "    all_measures[\"BPREF\"] = sum(results[q][\"BPREF\"] for q in results) / total_queries\n",
        "\n",
        "    # Optional extra credit averages\n",
        "    all_measures[\"P@29R\"] = sum(results[q][\"P@29R\"] for q in results) / total_queries\n",
        "    all_measures[\"P@R\"] = sum(results[q][\"P@R\"] for q in results) / total_queries\n",
        "\n",
        "    results[\"all\"] = all_measures\n",
        "\n",
        "    # Write results to the output file\n",
        "    report_measures(outputFile, results)\n"
      ],
      "metadata": {
        "id": "bNgda_3eR8VT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the evaluation"
      ],
      "metadata": {
        "id": "9U1WnOJdSVZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run the evaluation on the trecruns provided. Here's a breakdown of all the files we previously downloaded for this assignment. First, the `P2train.zip` file contains:\n",
        "\n",
        "- `msmarco.queries` is the actual queries that were used to generate the run files provided here and for which the qrels indicate what is relevant. You may find this useful or you may not. You do not need to use the file for the evaluation itself. It is provided in case you are curious.\n",
        "- `msmarco.qrels` is the list of which documents are relevant to which queries in the format described earlier. This file should be used for all of your evaluations in this project. Note that some queries will appear in this file but may not be in the trecrun output. In that case your evaluation output should not list those queries.\n",
        "- `msmarcosmall-{ql,bm25,dpr}.trecrun` are three ranked list files that you can use to try out your code.\n",
        "- `msmarcofull-{ql,bm25,dpr}.trecrun` are three files that you will evaluate without having the output provided to you. It is a superset of `msmarcosmall-*.trecrun`, so you could check that you get the same answers on the queries that are copied over.\n",
        "\n",
        "The file `P2train-output.zip` contains:\n",
        "- `msmarcosmall-{ql,bm25,dpr}.expeval` are the three output files corresponding to `msmarcosmall*.trecrun` showing what the output for each of those should be.\n",
        "\n",
        "Note that all of these files should have been successfully downloaded and unzipped in your `data_path` directory very early in this notebook."
      ],
      "metadata": {
        "id": "ybLuRBppHk0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "qrels_file = \"\"\n",
        "trecrun_files = []\n",
        "expected_output_files = []\n",
        "\n",
        "\n",
        "for filename in os.listdir(data_path):\n",
        "    if os.path.isfile(os.path.join(data_path, filename)):\n",
        "\n",
        "        if filename.endswith('.qrels'):\n",
        "          qrels_file = os.path.join(data_path, filename)\n",
        "        if filename.endswith('.trecrun'):\n",
        "          trecrun_files.append(os.path.join(data_path, filename))\n",
        "        if filename.endswith('.expeval'):\n",
        "          expected_output_files.append(os.path.join(data_path, filename))\n",
        "\n",
        "        print(filename)"
      ],
      "metadata": {
        "id": "jvgmtA2JDSuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235c8a6a-74d8-4049-b895-e742f1f8ba24"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P2train.zip\n",
            "msmarco.queries\n",
            "msmarcofull-ql.trecrun\n",
            "msmarcofull-dpr.trecrun\n",
            "msmarcofull-bm25.trecrun\n",
            "msmarcosmall-ql.trecrun\n",
            "msmarcosmall-dpr.trecrun\n",
            "msmarcosmall-bm25.trecrun\n",
            "msmarco.qrels\n",
            "P2train-output.zip\n",
            "msmarcosmall-bm25.expeval\n",
            "msmarcosmall-dpr.expeval\n",
            "msmarcosmall-ql.expeval\n",
            "msmarcofull-ql.eval\n",
            "msmarcofull-dpr.eval\n",
            "msmarcofull-bm25.eval\n",
            "msmarcosmall-ql.eval\n",
            "msmarcosmall-dpr.eval\n",
            "msmarcosmall-bm25.eval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can run the `eval` function for all trecrun files downloaded. (During debugging you may want to do just one of the files until things work.)"
      ],
      "metadata": {
        "id": "c-ULkNHxNnno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for trecrun_file in trecrun_files:\n",
        "  run_name = os.path.splitext(trecrun_file)[0]\n",
        "  output_file = run_name + '.eval'\n",
        "\n",
        "  eval(trecrun_file, qrels_file, output_file)\n"
      ],
      "metadata": {
        "id": "NBWNO1O7STn0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are encouraged to write additional code to test the output of your evaluation function and compare with the expected output files (or manually do so) before submitting to Gradescope. In particular, you might want to consider edge cases where something odd (but technically correct) is happening.\n",
        "\n",
        "Note that generating the output files is to help you debug your overall evaluation processing. The autograder will regenerate all of the output files itself, including some on data that you do not have access to."
      ],
      "metadata": {
        "id": "ZdxnIxWKMlka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Analysis questions\n",
        "\n",
        "## 4.1 Comparison table\n",
        "Create a table of the \"full\" QL, BM25, and DPR runs where the table has six columns as follows:\n",
        "\n",
        "* Column one is a query number\n",
        "* Column two is that query's AP performance on BM25 (%6.4f)\n",
        "* Column three is that query's AP for that query in QL (%6.4f)\n",
        "* Column four is the percent improvement from BM25 to QL (%5.1f)\n",
        "* Column five is the query's performance in DPR (%6.4f)\n",
        "* Column six is the percent improvement from BM25 to DPR (%5.1f)\n",
        "\n",
        "You can write whatever function you like to do this (we will grade the output and not test your function directly). It might be simplest to read in multiple trecrun files and evaluate them each to generate this information, most likely using your existing evaluation code.\n"
      ],
      "metadata": {
        "id": "aP8AeMh6wyD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def comparison_table():\n",
        "    \"\"\"\n",
        "    Create a comparison table for the \"full\" QL, BM25, and DPR runs.\n",
        "    The table includes six columns:\n",
        "      1. Query number\n",
        "      2. AP for BM25\n",
        "      3. AP for QL\n",
        "      4. Percent improvement from BM25 to QL\n",
        "      5. AP for DPR\n",
        "      6. Percent improvement from BM25 to DPR\n",
        "    \"\"\"\n",
        "    # Define the file paths for the full BM25, QL, and DPR trecrun files\n",
        "    bm25_file = \"/content/drive/MyDrive/COMPSCI446/msmarcofull-bm25.trecrun\"\n",
        "    ql_file = \"/content/drive/MyDrive/COMPSCI446/msmarcofull-ql.trecrun\"\n",
        "    dpr_file = \"/content/drive/MyDrive/COMPSCI446/msmarcofull-dpr.trecrun\"\n",
        "    qrels_file = \"/content/drive/MyDrive/COMPSCI446/msmarco.qrels\"\n",
        "\n",
        "    # Temporary output file for evaluation results\n",
        "    temp_output = \"temp_output.eval\"\n",
        "\n",
        "    # Function to evaluate and parse the results\n",
        "    def eval_file(file, qrels_file, output_file):\n",
        "        if not os.path.exists(file):\n",
        "            print(f\"Error: File {file} does not exist.\")\n",
        "            return {}\n",
        "\n",
        "        eval(file, qrels_file, output_file)  # Calling the eval function you provided\n",
        "\n",
        "        if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:\n",
        "            print(f\"Warning: Output file {output_file} is empty or not generated.\")\n",
        "            return {}\n",
        "\n",
        "        results = {}\n",
        "        with open(output_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                measure, query_id, score = line.split()\n",
        "                if measure == \"AP\":\n",
        "                    results[query_id] = float(score)\n",
        "        return results\n",
        "\n",
        "    # Evaluate BM25\n",
        "    bm25_results = eval_file(bm25_file, qrels_file, temp_output)\n",
        "\n",
        "    # Evaluate QL\n",
        "    ql_results = eval_file(ql_file, qrels_file, temp_output)\n",
        "\n",
        "    # Evaluate DPR\n",
        "    dpr_results = eval_file(dpr_file, qrels_file, temp_output)\n",
        "\n",
        "    # Check if results were retrieved\n",
        "    if not bm25_results or not ql_results or not dpr_results:\n",
        "        print(\"No results were processed. Please check your input files and evaluation process.\")\n",
        "        return\n",
        "\n",
        "    # Create the table\n",
        "    print(f\"{'Query':<10} {'BM25 AP':<10} {'QL AP':<10} {'BM25->QL (%)':<15} {'DPR AP':<10} {'BM25->DPR (%)':<15}\")\n",
        "    for query_id in bm25_results.keys():\n",
        "        bm25_ap = bm25_results.get(query_id, 0.0)\n",
        "        ql_ap = ql_results.get(query_id, 0.0)\n",
        "        dpr_ap = dpr_results.get(query_id, 0.0)\n",
        "\n",
        "        percent_improve_ql = ((ql_ap - bm25_ap) / bm25_ap * 100) if bm25_ap > 0 else 0.0\n",
        "        percent_improve_dpr = ((dpr_ap - bm25_ap) / bm25_ap * 100) if bm25_ap > 0 else 0.0\n",
        "\n",
        "        print(f\"{query_id:<10} {bm25_ap:<10.4f} {ql_ap:<10.4f} {percent_improve_ql:<15.1f} {dpr_ap:<10.4f} {percent_improve_dpr:<15.1f}\")\n",
        "\n",
        "\n",
        "# Call the function to print the table\n",
        "comparison_table()\n"
      ],
      "metadata": {
        "id": "183o9Q25yctg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a498584a-cac0-49d8-ac96-714a6f2e8b3a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query      BM25 AP    QL AP      BM25->QL (%)    DPR AP     BM25->DPR (%)  \n",
            "23849      0.0186     0.0151     -18.8           0.2391     1185.5         \n",
            "42255      0.2625     0.1987     -24.3           0.4411     68.0           \n",
            "47210      0.2021     0.1997     -1.2            0.3692     82.7           \n",
            "67316      0.0151     0.0080     -47.0           0.0853     464.9          \n",
            "118440     0.0048     0.0041     -14.6           0.0084     75.0           \n",
            "121171     0.6956     0.6916     -0.6            0.2195     -68.4          \n",
            "135802     0.1176     0.1164     -1.0            0.0546     -53.6          \n",
            "141630     0.4690     0.3959     -15.6           0.4309     -8.1           \n",
            "156498     0.0701     0.0564     -19.5           0.1348     92.3           \n",
            "169208     0.1075     0.1319     22.7            0.1028     -4.4           \n",
            "174463     0.0301     0.0041     -86.4           0.1838     510.6          \n",
            "258062     0.0317     0.0314     -0.9            0.1640     417.4          \n",
            "324585     0.0369     0.0449     21.7            0.3791     927.4          \n",
            "330975     0.2357     0.1675     -28.9           0.5441     130.8          \n",
            "332593     0.2310     0.2522     9.2             0.2221     -3.9           \n",
            "336901     0.0634     0.0634     0.0             0.1708     169.4          \n",
            "390360     0.2741     0.3275     19.5            0.2375     -13.4          \n",
            "405163     0.0735     0.0787     7.1             0.0013     -98.2          \n",
            "555530     0.0134     0.0084     -37.3           0.2544     1798.5         \n",
            "583468     0.7267     0.6706     -7.7            0.7084     -2.5           \n",
            "640502     0.0878     0.1199     36.6            0.1880     114.1          \n",
            "673670     0.0416     0.0324     -22.1           0.0009     -97.8          \n",
            "701453     0.5531     0.5663     2.4             0.3160     -42.9          \n",
            "730539     0.1356     0.2034     50.0            0.1628     20.1           \n",
            "768208     0.2433     0.2353     -3.3            0.0623     -74.4          \n",
            "877809     0.2516     0.1914     -23.9           0.2097     -16.7          \n",
            "911232     0.1542     0.2038     32.2            0.1592     3.2            \n",
            "914916     0.2860     0.3638     27.2            0.4061     42.0           \n",
            "938400     0.1043     0.1660     59.2            0.3848     268.9          \n",
            "940547     0.0892     0.0868     -2.7            0.3152     253.4          \n",
            "940548     0.0000     0.0000     0.0             0.0000     0.0            \n",
            "997622     0.0524     0.0748     42.7            0.1313     150.6          \n",
            "1030303    0.5014     0.5014     0.0             0.1939     -61.3          \n",
            "1037496    0.3181     0.4259     33.9            0.2945     -7.4           \n",
            "1043135    0.1031     0.1128     9.4             0.1281     24.2           \n",
            "1049519    0.0000     0.0000     0.0             0.0000     0.0            \n",
            "1051399    0.0113     0.0201     77.9            0.1348     1092.9         \n",
            "1056416    0.0000     0.0000     0.0             0.0000     0.0            \n",
            "1064670    0.2312     0.2233     -3.4            0.1521     -34.2          \n",
            "1071750    0.2685     0.2587     -3.6            0.2944     9.6            \n",
            "1103153    0.0000     0.0000     0.0             0.0000     0.0            \n",
            "1105792    0.3840     0.3999     4.1             0.1988     -48.2          \n",
            "1106979    0.5034     0.6340     25.9            0.5401     7.3            \n",
            "1108651    0.0250     0.0547     118.8           0.2464     885.6          \n",
            "1108729    0.0000     0.0000     0.0             0.0000     0.0            \n",
            "1109707    0.1750     0.1502     -14.2           0.1376     -21.4          \n",
            "1110678    0.3262     0.4205     28.9            0.0201     -93.8          \n",
            "1113256    0.4969     0.4953     -0.3            0.4651     -6.4           \n",
            "1115210    0.0887     0.0915     3.2             0.0651     -26.6          \n",
            "1116380    0.0111     0.0396     256.8           0.0587     428.8          \n",
            "1119543    0.0000     0.0000     0.0             0.0000     0.0            \n",
            "1121353    0.2349     0.2557     8.9             0.1002     -57.3          \n",
            "1122767    0.3235     0.3460     7.0             0.2052     -36.6          \n",
            "1127540    0.2764     0.2693     -2.6            0.1705     -38.3          \n",
            "1131069    0.0856     0.0288     -66.4           0.2143     150.4          \n",
            "1132532    0.1044     0.1666     59.6            0.2442     133.9          \n",
            "1133579    0.6666     0.6677     0.2             0.7530     13.0           \n",
            "1136043    0.1569     0.0976     -37.8           0.3695     135.5          \n",
            "1136047    0.0464     0.0666     43.5            0.0623     34.3           \n",
            "1136769    0.0000     0.0000     0.0             0.0000     0.0            \n",
            "1136962    0.4879     0.4689     -3.9            0.4199     -13.9          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Interpret comparison table\n",
        "What does that table tell you? We are in the process of talking about these models in class, but in case we haven't gotten to them or in case you forget... To a first approximation: QL is a straightforward estimate of term probability in a document; BM25 is a more complicated combination of features such as the frequency of a term in the document and across the collection, document length, and so on; and that DPR is the dense passage retrieval neural model. If it is helpful, the textbook talks about QL and BM25; it does not talk about DPR.\n",
        "\n",
        "Speculate on the reason for the results you are seeing."
      ],
      "metadata": {
        "id": "DoKzXJNP1XXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table compares the performance of BM25, QL, and DPR using Average Precision (AP).\n",
        "\n",
        "BM25 uses term frequency and document features for retrieval, often performing well for term-matching tasks.\n",
        "QL is simpler, focusing on term probabilities, and may perform similarly to BM25 for straightforward queries.\n",
        "DPR outperforms both by using deep learning to understand semantic meaning, making it more effective for complex queries.\n",
        "The results likely show that DPR excels due to its ability to capture deeper contextual relationships, while BM25 and QL are more effective for simpler queries where term matching is key."
      ],
      "metadata": {
        "id": "5OP4cMnj3x3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 AP with no documents\n",
        "\n",
        "How do you calculate AP of a query with no retrieved documents? (This is different than a query that retrieves only some of the relevant documents. Here, it retrieves _nothing_!) Why is that a hard question? Argue for what you think the answer should be."
      ],
      "metadata": {
        "id": "ZZrmnyDnyo6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Average Precision (AP) for a query with no retrieved documents should be 0.0. This is because AP is calculated based on the precision at each relevant document retrieved. If no documents are retrieved, there is no opportunity for any precision to be accumulated, resulting in an AP of zero.\n",
        "\n",
        "This is a hard question because, ideally, AP rewards systems that retrieve relevant documents. However, if no documents are retrieved, it's not clear how to handle this case without giving a misleading high score. Zero AP ensures that no retrieval is penalized too lightly.*Enter your answer here*"
      ],
      "metadata": {
        "id": "Z_da0nk1y9ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Recall / precision graph\n",
        "\n",
        "Plot a precision / recall graph for query 141630. Include data for QL, BM25, and DPR on the same graph. Note that there is not enough information to do this in the output described above -- you will have to generate additional information during the evaluation of query 141630 to accomplish this. You do not have to interpolate the values for this graph; you can plot them directly.\n",
        "\n",
        " We are providing you with the graphing code from P1's analysis questions. You may adapt it to this problem or you may use a completely different function (including one of your own!) if you prefer."
      ],
      "metadata": {
        "id": "8Mgu_ih9zMV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_precision_recall(query_info, total_relevant):\n",
        "    retrieved = query_info[\"retrieved\"]  # Documents retrieved by the model\n",
        "    relevant_found = query_info[\"relFound\"]  # Relevant documents retrieved\n",
        "\n",
        "    precision = relevant_fouadef calculate_precision_recall(query_info, total_relevant):\n",
        "    retrieved = query_info[\"retrieved\"]  # Documents retrieved by the model\n",
        "    relevant_found = query_info[\"relFound\"]  # Relevant documents retrieved\n",
        "\n",
        "    precision = relevant_found / retrieved if retrieved > 0 else 0.0\n",
        "    recall = relevant_found / total_relevant if total_relevant > 0 else 0.0\n",
        "    return precision, recall\n",
        "\n",
        "def get_total_relevant_documents(query_id, qrels_file):\n",
        "    \"\"\"Helper function to get total relevant documents from qrels file.\"\"\"\n",
        "    total_relevant = 0\n",
        "    with open(qrels_file, 'r') as f:\n",
        "        for line in f:\n",
        "            q_id, _, doc_id, relevance = line.strip().split()\n",
        "            if q_id == query_id and int(relevance) > 0:\n",
        "                total_relevant += 1\n",
        "    return total_relevant\n",
        "\n",
        "def get_precision_recall(query_id, query_info, qrels_file):\n",
        "    # Get the total relevant documents for the query from the qrels file\n",
        "    total_relevant = get_total_relevant_documents(query_id, qrels_file)\n",
        "\n",
        "    # Get precision and recall for BM25, QL, and DPR\n",
        "    bm25_precision, bm25_recall = calculate_precision_recall(query_info[\"bm25\"], total_relevant)\n",
        "    ql_precision, ql_recall = calculate_precision_recall(query_info[\"ql\"], total_relevant)\n",
        "    dpr_precision, dpr_recall = calculate_precision_recall(query_info[\"dpr\"], total_relevant)\n",
        "\n",
        "    return {\n",
        "        'BM25': (bm25_precision, bm25_recall),\n",
        "        'QL': (ql_precision, ql_recall),\n",
        "        'DPR': (dpr_precision, dpr_recall)\n",
        "    }\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_precision_recall(precision_recall_data):\n",
        "    models = list(precision_recall_data.keys())\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    for model in models:\n",
        "        precision, recall = zip(*precision_recall_data[model])\n",
        "        plt.plot(recall, precision, label=model)\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision vs Recall for Query 141630')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# --- Define query_info with sample data ---\n",
        "query_info = {\n",
        "    \"bm25\": {\"retrieved\": 10, \"relFound\": 5},\n",
        "    \"ql\": {\"retrieved\": 12, \"relFound\": 6},\n",
        "    \"dpr\": {\"retrieved\": 8, \"relFound\": 4}\n",
        "}\n",
        "# --- End of sample data definition ---\n",
        "\n",
        "# Get precision and recall data for query 141630\n",
        "precision_recall_data = get_precision_recall('141630', query_info, qrels_file)\n",
        "\n",
        "# Plot the graph\n",
        "plot_precision_recall(precision_recall_data)nd / retrieved if retrieved > 0 else 0.0\n",
        "    recall = relevant_found / total_relevant if total_relevant > 0 else 0.0\n",
        "    return precision, recall\n",
        "def get_precision_recall(query_id, query_info, qrels_file):\n",
        "    # Get the total relevant documents for the query from the qrels file\n",
        "    total_relevant = get_total_relevant_documents(query_id, qrels_file)\n",
        "\n",
        "    # Get precision and recall for BM25, QL, and DPR\n",
        "    bm25_precision, bm25_recall = calculate_precision_recall(query_info[\"bm25\"], total_relevant)\n",
        "    ql_precision, ql_recall = calculate_precision_recall(query_info[\"ql\"], total_relevant)\n",
        "    dpr_precision, dpr_recall = calculate_precision_recall(query_info[\"dpr\"], total_relevant)\n",
        "\n",
        "    return {\n",
        "        'BM25': (bm25_precision, bm25_recall),\n",
        "        'QL': (ql_precision, ql_recall),\n",
        "        'DPR': (dpr_precision, dpr_recall)\n",
        "    }\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_precision_recall(precision_recall_data):\n",
        "    models = list(precision_recall_data.keys())\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    for model in models:\n",
        "        precision, recall = zip(*precision_recall_data[model])\n",
        "        plt.plot(recall, precision, label=model)\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision vs Recall for Query 141630')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Get precision and recall data for query 141630\n",
        "precision_recall_data = get_precision_recall('141630', query_info, qrels_file)\n",
        "\n",
        "# Plot the graph\n",
        "plot_precision_recall(precision_recall_data)\n"
      ],
      "metadata": {
        "id": "R2uzyGDuzlh1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "44e5939e-a454-4193-f739-b28021ebb67a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-46-8364a6984aa1>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-8364a6984aa1>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    precision = relevant_fouadef calculate_precision_recall(query_info, total_relevant):\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Recall / precision interpolation (extra credit)\n",
        "\n",
        "Repeat 4.4 but graph it for recall values of 0.0 through 1.0 at intervals of 0.1, with the precision values interpolated as presented in class.\n",
        "\n",
        "Depending on how you implemented the graphing function for 4.4, you may be able to use the same one you used above."
      ],
      "metadata": {
        "id": "5VQR0VIdz3kM"
      }
    },
    {
      "source": [
        "def calculate_precision_recall(query_info, total_relevant):\n",
        "    retrieved = query_info[\"retrieved\"]  # Documents retrieved by the model\n",
        "    relevant_found = query_info[\"relFound\"]  # Relevant documents retrieved\n",
        "\n",
        "    precision = relevant_found / retrieved if retrieved > 0 else 0.0\n",
        "    recall = relevant_found / total_relevant if total_relevant > 0 else 0.0\n",
        "    return precision, recall\n",
        "\n",
        "def get_total_relevant_documents(query_id, qrels_file):\n",
        "    \"\"\"Helper function to get total relevant documents from qrels file.\"\"\"\n",
        "    total_relevant = 0\n",
        "    with open(qrels_file, 'r') as f:\n",
        "        for line in f:\n",
        "            q_id, _, doc_id, relevance = line.strip().split()\n",
        "            if q_id == query_id and int(relevance) > 0:\n",
        "                total_relevant += 1\n",
        "    return total_relevant\n",
        "\n",
        "def get_precision_recall(query_id, query_info, qrels_file):\n",
        "    # Get the total relevant documents for the query from the qrels file\n",
        "    total_relevant = get_total_relevant_documents(query_id, qrels_file)\n",
        "\n",
        "    # Get precision and recall for BM25, QL, and DPR\n",
        "    bm25_precision, bm25_recall = calculate_precision_recall(query_info[\"bm25\"], total_relevant)\n",
        "    ql_precision, ql_recall = calculate_precision_recall(query_info[\"ql\"], total_relevant)\n",
        "    dpr_precision, dpr_recall = calculate_precision_recall(query_info[\"dpr\"], total_relevant)\n",
        "\n",
        "    return {\n",
        "        'BM25': (bm25_precision, bm25_recall),\n",
        "        'QL': (ql_precision, ql_recall),\n",
        "        'DPR': (dpr_precision, dpr_recall)\n",
        "    }\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_precision_recall(precision_recall_data):\n",
        "    models = list(precision_recall_data.keys())\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    for model in models:\n",
        "        precision, recall = precision_recall_data[model]  # Directly assign precision and recall\n",
        "        plt.plot([recall], [precision], label=model, marker='o')  # Plot as single points\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision vs Recall for Query 141630')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "query_info = {\n",
        "    \"bm25\": {\"retrieved\": 10, \"relFound\": 5},\n",
        "    \"ql\": {\"retrieved\": 12, \"relFound\": 6},\n",
        "    \"dpr\": {\"retrieved\": 8, \"relFound\": 4}\n",
        "}\n",
        "\n",
        "# Get precision and recall data for query 141630\n",
        "precision_recall_data = get_precision_recall('141630', query_info, qrels_file)\n",
        "\n",
        "# Plot the graph\n",
        "plot_precision_recall(precision_recall_data)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "1R7qux_46iII",
        "outputId": "d8bd52c7-a965-4f7b-bc0e-abbb6ba8600b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92M-bw8hTEPd"
      },
      "source": [
        "# 5. Misc & Grading\n",
        "\n",
        "**WARNING: Points breakdown may shift with a later version. This gives you an idea of what we are expecting to do.**\n",
        "\n",
        "Your P2 submission is graded out of 100 points, allocated as follows:\n",
        "* 75 points for the code (these are autograded)\n",
        "  * 6 points: 3 points each for correctly calculating `numRel` and `relFound` on all six run files\n",
        "  * 24 points: 6 points each for correctly calculating `RR`, `P@13`, `R@13`, and `F1@13` on all six run files\n",
        "  * 45 points: 15 points each for correctly calculating `NDCG@23`, `AP`, and `BPREF` on all six run files\n",
        "  * Up to 6 points extra credit for P@29R and P@R on all six run files\n",
        "\n",
        "* 25 points for the analysis questions (these are graded manually):\n",
        "  * 10 points for correctness of generated table (4.1)\n",
        "  * 2 points for interpretation of generated table (4.2)\n",
        "  * 3 points for discussion of MAP (4.3)\n",
        "  * 10 points for correctness of recall/precision graph (4.4)\n",
        "  * Up to 4 points for extra credit interpolation graph (4.5)\n",
        "\n",
        "Note that we expect that you will upload your submission in the correct format (notebook and PDF), that the code in the notebook will run, and that the code will successfully process provided trecrun files, possibly including some you do not have access to."
      ]
    }
  ]
}